{"title":"Hive安装","slug":"hive_install","date":"2018-02-06T08:57:00.000Z","updated":"2018-04-18T09:45:30.625Z","comments":true,"excerpt":"","content":"<h3 id=\"目标\"><a href=\"#目标\" class=\"headerlink\" title=\"目标\"></a>目标</h3><p>安装一个基本的Hive环境,由于基本配置的安装,暂时先使用Hive自带的Derby作为元数据存储数据库,当然完全体还是要使用Mysql之类的数据库</p>\n<h3 id=\"环境\"><a href=\"#环境\" class=\"headerlink\" title=\"环境:\"></a>环境:</h3><p>Ubuntu 16.04 Server<br>已部署Hadoop2.8.3并启动<br><a href=\"http://wincher.cn\">详情</a></p>\n<h3 id=\"下载Apache-Hive\"><a href=\"#下载Apache-Hive\" class=\"headerlink\" title=\"下载Apache Hive\"></a>下载Apache Hive</h3><blockquote>\n<p><a href=\"http://hive.apache.org/downloads.html\" target=\"_blank\" rel=\"external\">下载地址</a><br>我选择了下载2.3.2版本</p>\n</blockquote>\n<h3 id=\"安装Apache-Hive\"><a href=\"#安装Apache-Hive\" class=\"headerlink\" title=\"安装Apache Hive\"></a>安装Apache Hive</h3><ol>\n<li>解压缩Hive<br> <code>sudo tar -zxvf apache-hive-2.3.2-bin.tar.gz</code></li>\n<li>讲解压的apache-hive-2.3.2-bin文件夹移动到/usr/local下并更名为hive<br><br> <code>mv apache-hive-2.3.2-bin /usr/local/hive</code></li>\n<li>解压Hive<br> <code>sudo tar -zxvf apache-hive-2.3.2-bin.tar.gz</code></li>\n<li>配置环境变量<br> <code>vim /etc/profile</code><br> 在文件尾部添加  <pre><code> export HIVE_HOME=/usr/local/hive\n export PATH=$PATH:$HIVE_HOME/bin\n</code></pre> 执行source命令是配置生效<br> <code>source /etc/profile</code></li>\n</ol>\n<h3 id=\"Hive配置Hadoop-HDFS\"><a href=\"#Hive配置Hadoop-HDFS\" class=\"headerlink\" title=\"Hive配置Hadoop HDFS\"></a>Hive配置Hadoop HDFS</h3><ol>\n<li>进入$HIVE_HOME/conf,复制hive-env.sh.template为hive-env.sh<br> <code>cd $HIVE_HOME/conf</code><br> <code>cp hive-env.sh.template hive-env.sh</code></li>\n<li>编辑hive-site.xml配置文件<br> <code>vim hive-env.sh</code><br> 添加如下内容<pre><code> export HADOOP_HOME=/usr/local/hadoop &lt;!-- 你的Hadoop的位置 --&gt;\n export HIVE_CONF_DIR=/usr/local/hive/conf\n export HIVE_AUX_JARS_PATH=/usr/local/hive/lib\n</code></pre></li>\n<li>进入$HIVE_HOME/conf,复制hive-default.xml.template为hive-site.xml<pre><code> cd $HIVE_HOME/conf  \n cp hive-default.xml.template hive-site.xml\n</code></pre></li>\n<li><p>编辑hive-site.xml配置文件,替换所有的{$system:java.io.tmpdir}为/usr/local/hive/tmp<br> 替换所有的{$system:user.name}为root</p>\n<pre><code> vim hive-site.xml\n :%s/{$system:java.io.tmpdir}/\\/user\\/local\\/hive\\/tmp/g\n :%s/{$system:user.name}/root/g\n :wq\n</code></pre><p> 并修改如下几个配置：<br> metastore路径</p>\n<pre><code> &lt;property&gt;\n     &lt;name&gt;javax.jdo.option.ConnectionURL&lt;/name&gt;\n     &lt;value&gt;jdbc:derby:;databaseName=/usr/local/hive/bin/metastore_db;create=true&lt;/value&gt;\n     &lt;description&gt;\n       JDBC connect string for a JDBC metastore.\n       To use SSL to encrypt/authenticate the connection, provide database-specific SSL flag in the connection URL.\n       For example, jdbc:postgresql://myhost/db?ssl=true for postgres database.\n     &lt;/description&gt;\n   &lt;/property&gt;\n</code></pre><p> hdfs中的路径(这个我们后面要在Hdfs中创建)</p>\n<pre><code> &lt;property&gt;\n     &lt;name&gt;hive.exec.scratchdir&lt;/name&gt;\n     &lt;value&gt;/tmp/hive&lt;/value&gt;\n     &lt;description&gt;HDFS root scratch dir for Hive jobs which gets created with write all (733) permission. For each connecting user, an HDFS scratch dir: ${hive.exec.scratchdir}/&amp;lt;username&amp;gt; is created, with ${hive.scratch.dir.permission}.\n     &lt;/description&gt;\n &lt;/property&gt;\n</code></pre><p> 本地路径</p>\n<pre><code> &lt;property&gt;\n     &lt;name&gt;hive.exec.local.scratchdir&lt;/name&gt;\n     &lt;value&gt;/usr/local/hive/tmp/root&lt;/value&gt;\n     &lt;description&gt;Local scratch space for Hive jobs&lt;/description&gt;\n &lt;/property&gt;\n</code></pre></li>\n<li><p>使用Hadoop新建hdfs目录,对用hive-site.xml中有如下配置：</p>\n<pre><code> &lt;property&gt;\n     &lt;name&gt;hive.metastore.warehouse.dir&lt;/name&gt;\n     &lt;value&gt;/user/hive/warehouse&lt;/value&gt;\n     &lt;description&gt;location of default database for the warehouse&lt;/description&gt;\n     &lt;/property&gt;\n &lt;property&gt;\n</code></pre><p> 执行hadoop命令新建/user/hive/warehouse目录<br> 给新建的目录赋予读写权限<br> 查看修改后的权限  </p>\n<pre><code> hadoop dfs -mkdir -p /user/hive/warehouse\n hdfs dfs -chmod 777 /user/hive/warehouse\n hdfs dfs -ls /user/hive\n Found 1 items\n drwxrwxrwx   - root supergroup          0 2018-02-06 16:05 /user/hive/warehouse\n</code></pre><p> 运用hadoop命令新建/tmp/hive目录(这个对应上面的配置hive.exec.scratchdir)<br> 给目录/tmp/hive赋予读写权限<br> 检查创建好的目录  </p>\n<pre><code> hdfs dfs -mkdir -p /tmp/hive  \n hdfs dfs -chmod 777 /tmp/hive\n hdfs dfs -ls /tmp\n Found 1 items\n drwxrwxrwx   - root supergroup          0 2018-02-06 17:04 /tmp/hive\n</code></pre></li>\n</ol>\n<h3 id=\"使用HIVE\"><a href=\"#使用HIVE\" class=\"headerlink\" title=\"使用HIVE\"></a>使用HIVE</h3><ol>\n<li>初始化derby<br> schematool -initSchema -dbType derby</li>\n<li><p>使用(注意使用前需要启动Hadoop的Hdfs和Yarn，start-all.sh包含了这两个的启动)<br> 命令行输入,会出现HQL命令行提示符(会有几行log)</p>\n<pre><code> root@master:/usr/local/hive/conf# hive\n hive&gt;\n</code></pre><p> 这样就安装完成了，接下来是一些简单的操作:</p>\n<ol>\n<li>查看函数列表<br><code>hive&gt;show functions;</code></li>\n<li>查看某个函数的详细信息如:sum<pre><code>hive&gt; desc function sum;\nOK\nsum(x) - Returns the sum of a set of numbers\nTime taken: 0.013 seconds, Fetched: 1 row(s)\n</code></pre></li>\n<li>新建库并使用  <pre><code>hive&gt; create database test;\nOK\nTime taken: 0.448 seconds\n</code></pre></li>\n<li>新建数据表并查看详情  <pre><code>hive&gt; create table person(id int, name string) row format delimited fields terminated by &#39;\\t&#39;;\nOK\nTime taken: 0.16 seconds\nhive&gt; desc person;\nOK\nid                      int\nname                    string\nTime taken: 0.083 seconds, Fetched: 2 row(s)\n</code></pre></li>\n<li><p>将文件写入表中<br>在$HIVE_HOME下新建文件person.date,并在文件中添加如下内容,<br>注意id和name间是TAB键,我们在建表语句中用了terminated by ‘\\t’,所以这个分割是必须要用TAB键的</p>\n<pre><code>001    Tiny\n002    Lina\n003    Zues\n004    Sven\n005    Riki\n006    Park\n007    Morphling\n008    Riki\n009    Morphling\n010    Morphling\n</code></pre></li>\n<li>将数据倒入并查看是否成功<pre><code>hive&gt; load data local inpath &#39;/usr/local/hive/person.dat&#39; into table test.person;\nLoading data to table test.person\nOK\nTime taken: 9.767 seconds\nhive&gt; select * from test.person;\nOK\n1    Tiny\n2    Lina\n3    Zues\n4    Sven\n5    Riki\n6    Park\n7    Morphling\n8    Riki\n9    Morphling\n10    Morphling\nTime taken: 2.671 seconds, Fetched: 10 row(s)\n</code></pre></li>\n<li>在Hadoop的NameNode上也能查看到刚写入HDFS的数据person.dat<br><a href=\"http://master:50070/explorer.html#/user/hive/warehouse/test.db/person\" target=\"_blank\" rel=\"external\">http://master:50070/explorer.html#/user/hive/warehouse/test.db/person</a><br><img src=\"/2018/02/06/hive_install/ScreenShot20180206at163905.png\" alt=\"pic\"></li>\n</ol>\n</li>\n</ol>\n","categories":[{"name":"Hive","path":"api/categories/Hive.json"}],"tags":[{"name":"Hive","path":"api/tags/Hive.json"}]}